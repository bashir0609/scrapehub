# ScrapeHub ğŸš€

<div align="center">

**A powerful, multi-platform web scraping application built with Django**

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Django](https://img.shields.io/badge/Django-4.2.7-green.svg)](https://www.djangoproject.com/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-15-blue.svg)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Documentation](#-documentation) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸ“– Overview

ScrapeHub is a comprehensive web scraping platform that provides a unified interface for extracting data from various sources. Whether you need to scrape APIs, websites, social media platforms, or e-commerce sites, ScrapeHub has you covered.

### Why ScrapeHub?

- ğŸ¯ **Universal API Client** - Scrape any API endpoint with ease
- ğŸ” **Company Social Finder** - Extract company information and social media profiles
- ğŸ›’ **E-commerce Scraper** - Generic scraper that works with any e-commerce website
- âš¡ **RapidAPI Integration** - Access thousands of APIs through RapidAPI marketplace (Coming Soon)
- ğŸ“ **Ads.txt Checker** - Bulk validate ads.txt and app-ads.txt files
- ğŸ“± **Social Media Scraper** - Extract data from social platforms (Coming Soon)
- ğŸ¨ **Modern UI** - Beautiful, responsive web interface
- ğŸ³ **Docker Ready** - Easy deployment with Docker Compose
- ğŸ“Š **Progress Tracking** - Real-time progress monitoring for bulk operations
- ğŸ’¾ **Export Options** - CSV, JSON export capabilities

---

## âœ¨ Features

### âœ… Universal API Client (Fully Implemented)

- ğŸš€ **Any API Support** - Scrape any API endpoint (POST, GET, PUT, DELETE)
- ğŸ“ **Request History** - Store and view all scraping requests in database
- ğŸ¨ **Modern Interface** - Clean, responsive web UI
- ğŸ”’ **Error Handling** - Comprehensive error handling and validation
- ğŸ“Š **History View** - View and manage scraping request history
- ğŸ“‹ **Network Tab Import** - Import headers and payload from browser Network tab
- ğŸ¯ **Field Selection** - Select and filter specific fields from responses
- ğŸ“„ **Pagination** - Automatic pagination support
- ğŸ’¾ **Export** - CSV/JSON export functionality
- ğŸ” **Dynamic Fields** - Automatic field extraction from API responses
- ğŸ” **Dynamic Fields** - Automatic field extraction from API responses

### âœ… Ads.txt Checker (Fully Implemented)

A powerful tool for bulk validation of **ads.txt** and **app-ads.txt** files.

- ğŸ” **Bulk Validation** - Validate thousands of domains at once
- ğŸ“‚ **Auto-Discovery** - Automatically finds homepage and ads.txt location
- ğŸ“ **Content Analysis** - detailed analysis of ads.txt content
- ğŸ“Š **Status Codes** - Checks for 200 OK, 403, 404, etc.
- ğŸš€ **Async Processing** - Fast parallel processing using Django-Q
- ğŸ“‹ **Live Results** - Real-time progress tracking
- ğŸ’¾ **Export** - Export validation results as CSV/JSON

**URL**: `/ads-txt-checker/`

Formerly known as "Web Scraper", this tool focuses on finding company information and social media profiles.

- ğŸŒ **HTML Parsing** - BeautifulSoup for HTML parsing
- ğŸ¯ **CSS Selectors** - Extract data using CSS selectors
- ğŸ” **XPath Support** - XPath expression support
- âš™ï¸ **JavaScript Rendering** - Selenium & Playwright for JS-heavy sites
- ğŸ“¦ **Bulk Scraping** - Scrape multiple URLs at once
- ğŸ“ **File Upload** - Upload CSV/TXT files with URL lists
- ğŸ“Š **Progress Tracking** - Real-time progress monitoring
- ğŸ”„ **Pagination** - Handle paginated content
- ğŸ“‹ **Table Extraction** - Extract tables from web pages
- ğŸ”— **Link & Image Extraction** - Extract all links and images
- ğŸ“ **Structured Data** - Extract JSON-LD and microdata
- ğŸ’¾ **Export** - CSV/JSON export

**URL**: `/company-social-finder/`

### âœ… E-commerce Scraper (Fully Implemented)

A **generic e-commerce scraper** that works with **ANY e-commerce website** - no platform restrictions!

- ğŸŒ **Universal Support** - Works with any e-commerce site (Amazon, eBay, Shopify, AliExpress, Daraz, etc.)
- ğŸ¨ **Visual Selector Builder** - Click elements to generate CSS selectors
- âš™ï¸ **Custom Selectors** - Configure CSS selectors for any site
- ğŸ“¦ **Listing Pages** - Scrape category, search, tag, and collection pages
- ğŸ”„ **Pagination** - Automatic pagination handling
- ğŸ’° **Price Tracking** - Historical price tracking
- ğŸ“Š **Product Data** - Extract title, price, images, ratings, reviews
- ğŸ¯ **Product Cards** - Extract multiple products from listing pages
- ğŸ” **Anti-Bot Measures** - Configuration framework for stealth scraping
- ğŸ“ˆ **Price History** - Track price changes over time
- ğŸ’¾ **Export** - Export product data and price history

**URL**: `/ecommerce-scraper/`

### ğŸš§ Coming Soon

- ğŸ“± **Social Scraper** - Extract data from Twitter, Instagram, Facebook, LinkedIn, etc.
- âš¡ **RapidAPI Scraper** - Browse and execute thousands of APIs from RapidAPI marketplace

---

## ğŸš€ Quick Start

### Using Docker (Recommended)

1. **Clone the repository**:

```bash
git clone <repository-url>
cd "API Scraper"
```

2. **Start the application**:

```bash
docker-compose up --build
```

3. **Open your browser**:

```
http://localhost:8001
```

**Note**: The app runs on port `8001` when using Docker (mapped from container port 8000).

### Manual Installation

1. **Create virtual environment**:

```bash
python -m venv venv
```

2. **Activate virtual environment**:

   - **Windows**: `venv\Scripts\activate`
   - **Linux/Mac**: `source venv/bin/activate`

3. **Install dependencies**:

```bash
pip install -r requirements.txt
```

4. **Run migrations**:

```bash
python manage.py migrate
```

5. **Create superuser** (optional):

```bash
python manage.py createsuperuser
```

6. **Run development server**:

```bash
python manage.py runserver
```

7. **Open browser**:

```
http://127.0.0.1:8000
```

---

## ğŸ“ Project Structure

```
scrapehub/                          # Django project folder
â”œâ”€â”€ scrapehub/                      # Project settings
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ urls.py
â”‚   â””â”€â”€ wsgi.py
â”‚
â”œâ”€â”€ scrapers/                       # All scrapers organized in one folder
â”‚   â”œâ”€â”€ universal_api/              # Universal API Client
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ views.py
â”‚   â”‚   â””â”€â”€ urls.py
â”‚   â”‚
â”‚   â”œâ”€â”€ company_social_finder/      # Company Social Finder
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”œâ”€â”€ views.py
â”‚   â”‚   â””â”€â”€ urls.py
â”‚   â”‚
â”‚   â””â”€â”€ ecommerce_scraper/         # E-commerce Scraper
â”‚       â”œâ”€â”€ models.py
â”‚       â”œâ”€â”€ views.py
â”‚       â””â”€â”€ scraper_helpers.py
â”‚
â”œâ”€â”€ templates/                      # HTML templates
â”‚   â”œâ”€â”€ index.html                 # Home page
â”‚   â””â”€â”€ scrapers/                   # Scraper templates
â”‚       â”œâ”€â”€ company_social_finder.html
â”‚       â”œâ”€â”€ ecommerce_scraper.html
â”‚       â”œâ”€â”€ rapidapi_scraper.html
â”‚       â””â”€â”€ social_scraper.html
â”‚
â”œâ”€â”€ static/                         # Development static files
â”œâ”€â”€ media/                          # User uploads
â”œâ”€â”€ manage.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ’» Usage

### Web Interface

1. Navigate to `http://localhost:8001` (or `http://127.0.0.1:8000` for local)
2. Choose your scraper:
   - **Universal API Client** - `/` (home page)
   - **Company Social Finder** - `/company-social-finder/`
   - **E-commerce Scraper** - `/ecommerce-scraper/`
   - **Ads.txt Checker** - `/ads-txt-checker/`
3. Configure your scraping parameters
4. Click "Scrape" and view results
5. Export data as CSV or JSON

### API Endpoints

#### Universal API Client

**Endpoint**: `POST /api/scrape/`

**Request**:

```json
{
  "url": "https://example.com/api/endpoint",
  "method": "POST",
  "data": {
    "current": 1,
    "size": 10
  },
  "headers": {
    "Content-Type": "application/json"
  }
}
```

**Response**:

```json
{
  "success": true,
  "status_code": 200,
  "data": {
    /* API response */
  },
  "request_id": 1
}
```

#### Company Social Finder

**Single Page Scraping**: `POST /api/web-scrape/`

**Bulk Scraping**: `POST /api/web-scrape-bulk/`

**Progress Tracking**: `GET /api/web-scrape-progress/?request_id={id}`

**Bulk Results**: `GET /api/web-scrape-bulk-results/?request_id={id}`

#### E-commerce Scraper

**Generic Scraping**: `POST /api/ecommerce-scrape/`

**Price Tracking**: `POST /api/ecommerce-price-track/`

**Price History**: `GET /api/ecommerce-price-history/?product_id={id}`

#### Ads.txt Checker

**Submit Job**: `POST /ads-txt-checker/api/submit-job/`

**Request**:

```json
{
  "urls": ["example.com", "nytimes.com", "cnn.com"]
}
```

**Check Status**: `GET /jobs/api/status/{job_id}/`

**Get Results**: `GET /jobs/api/results/{job_id}/`

---

## ğŸ³ Docker Commands

```bash
# Start containers
docker-compose up

# Start in background
docker-compose up -d

# Stop containers
docker-compose down

# View logs
docker-compose logs -f

# Rebuild after code changes
docker-compose up --build

# Create superuser
docker-compose exec web python manage.py createsuperuser

# Run migrations
docker-compose exec web python manage.py migrate

# Access Django shell
docker-compose exec web python manage.py shell

# Access database
docker-compose exec db psql -U postgres -d scrapehub
```

### Production Deployment

For production, use the production docker-compose file:

```bash
docker-compose -f docker-compose.prod.yml up -d
```

**Environment Variables** (set in `.env` or docker-compose):

- `SECRET_KEY`: Django secret key
- `POSTGRES_PASSWORD`: Database password
- `DEBUG`: Set to `False` in production

---

## ğŸ”§ Configuration

### Database

The application uses:

- **SQLite** for local development (default)
- **PostgreSQL** when running in Docker or when `POSTGRES_DB` environment variable is set

### Static Files

- **Development**: Static files served from `static/` folder
- **Production**: Run `python manage.py collectstatic` to collect static files to `staticfiles/`

### Media Files

User uploads (bulk URL files) are stored in `media/bulk_inputs/`

---

## ğŸ“š Documentation

- **[Implementation Plan](./SCRAPER_IMPLEMENTATION_PLAN.md)** - Detailed roadmap and feature specifications
- **API Documentation** - Available in the web interface
- **Admin Panel** - Access at `/admin/` (requires superuser)

---

## ğŸ› ï¸ Technologies Used

- **Backend**: Django 4.2.7
- **Database**: PostgreSQL 15 / SQLite
- **Web Scraping**: BeautifulSoup4, lxml, Selenium, Playwright
- **HTTP Requests**: Requests
- **Containerization**: Docker & Docker Compose
- **Frontend**: HTML, CSS, JavaScript (Vanilla)

### Key Dependencies

```
Django==4.2.7
requests==2.31.0
beautifulsoup4>=4.12.0
lxml>=4.9.0
selenium>=4.15.0
playwright>=1.40.0
django-cors-headers==4.3.1
fake-useragent>=1.4.0
```

See `requirements.txt` for complete list.

---

## ğŸ—ºï¸ Roadmap

See [SCRAPER_IMPLEMENTATION_PLAN.md](./SCRAPER_IMPLEMENTATION_PLAN.md) for detailed implementation roadmap.

### Current Status

- âœ… Universal API Client - Fully implemented
- âœ… Company Social Finder - Core features complete
- âœ… E-commerce Scraper - Fully implemented (generic)
- âœ… Ads.txt Checker - Fully implemented
- ğŸš§ Social Scraper - Planned
- ğŸš§ RapidAPI Scraper - Planned

### Upcoming Features

- Advanced authentication handling
- Proxy support and rotation
- User-Agent rotation
- CAPTCHA solving integration
- Scheduled scraping tasks
- Real-time notifications
- Advanced analytics dashboard

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## âš ï¸ Legal & Ethical Considerations

- **Respect robots.txt** - Always check and respect website robots.txt files
- **Rate Limiting** - Implement appropriate delays between requests
- **Terms of Service** - Review and comply with each platform's ToS
- **Data Privacy** - Handle scraped data responsibly
- **Anti-Bot Measures** - Some sites may block automated scraping

**Disclaimer**: This tool is for educational and legitimate business purposes only. Users are responsible for ensuring their use complies with applicable laws and website terms of service.

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Django community for the excellent framework
- BeautifulSoup, Selenium, and Playwright teams for scraping tools
- All contributors and users of this project

---

## ğŸ“ Support

For issues, questions, or contributions:

- Open an issue on GitHub
- Check the [Implementation Plan](./SCRAPER_IMPLEMENTATION_PLAN.md) for detailed documentation

---

<div align="center">

**Made with â¤ï¸ for the scraping community**

â­ Star this repo if you find it useful!

</div>
